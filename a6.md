### Backpropagation

Para a atualização dos pesos \(\varpi_i^L\) da camada \(L\) nós já temos nossa resolução. Tal resolução é de fácil dedução pelo fato de nós termos um valor de referência \(\hat{y_i}\) para o output da última camada \(y_i\). Com base nisso, através da função de custo \(\psi\) é claro a inferência do erro nos pesos \(\varpi_i^L\) onde \(y_i = \varphi(\varpi_i^L \cdot A^{L-1[b]})\). 

Agora a nosso objetivo é observar o comportamento do erro informado pela função custo com base nos pesos \(\varpi_h^{L-1}\), tal que \(h=1, \dots, k_{L-1}\). Logo:

\[
Y = \begin{bmatrix} \varphi(\varpi_1^L \cdot A^{L-1[b]}) \\ \vdots \\ \varphi(\varpi_m^L \cdot A^{L-1[b]}) \end{bmatrix}, \quad A^{L-1[b]} = \begin{bmatrix} 1 \\ \varphi(\varpi_1^{L-1} \cdot A^{L-2[b]}) \\ \vdots \\ \varphi(\varpi_{k_{L-1}}^{L-1} \cdot A^{L-2[b]}) \end{bmatrix}
\]

Logo, vemos que, cada elemento \(y_i\) de \(Y\), onde \(y_i = \varphi(\varpi_i^L \cdot A^{L-1[b]})\), depende estritamente de qualquer \(\varpi_h^{L-1}\) devido ao fato de que todos os elementos \(y_i\) contém \(A^{L-1[b]}\) que contém um \(\varpi_h^{L-1}\).

Então definindo nossa função custo, temos:

\[
\psi_{\text{MSE}} = \frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2, \quad y_i = a_i^L= \varphi(z_i^L)= \varphi\left (\varpi_i^L \cdot \begin{bmatrix} 1 \\ \varphi(\varpi_1^{L-1} \cdot A^{L-2[b]}) \\ \vdots \\ \varphi(\varpi_{k_{L-1}}^{L-1} \cdot A^{L-2[b]})  \end{bmatrix} \right )
\]

Sendo assim podemos definir a variação da função custo com base em \(\varpi_h^{L-1}\). Definimos \(E_i = (y_i - \hat{y_i})^2\), então:

\[
\frac{\partial \psi_{\text{MSE}}}{\partial \varpi_h^{L-1}} = \frac{1}{m} \sum_{i=1}^{m} \frac{\partial E_i }{\partial a_i^L}\frac{\partial a_i^L }{\partial z_i^L}\frac{\partial z_i^L }{\partial a_h^{L-1}}\frac{\partial a_h^{L-1} }{\partial z_h^{L-1}}\frac{\partial z_h^{L-1} }{\partial \varpi_h^{L-1}}
\]