### Backpropagation

Para a atualização dos pesos \(\varpi_i^L\) da camada \(L\) nós já temos nossa resolução. Tal resolução é de fácil dedução pelo fato de nós termos um valor de referência \(\hat{y_i}\) para o output da última camada \(y_i\). Com base nisso, através da função de custo \(\psi\) é claro a inferência do erro nos pesos \(\varpi_i^L\) onde \(y_i = \varphi(\varpi_i^L \cdot A^{L-1[b]})\). 

Agora a nosso objetivo é observar o comportamento do erro informado pela função custo com base nos pesos \(\varpi_h^{L-1}\), tal que \(h=1, \dots, k_{L-1}\). Logo:

\[
Y = \begin{bmatrix} \varphi(\varpi_1^L \cdot A^{L-1[b]}) \\ \vdots \\ \varphi(\varpi_m^L \cdot A^{L-1[b]}) \end{bmatrix}, \quad A^{L-1[b]} = \begin{bmatrix} 1 \\ \varphi(\varpi_1^{L-1} \cdot A^{L-2[b]}) \\ \vdots \\ \varphi(\varpi_{k_{L-1}}^{L-1} \cdot A^{L-2[b]}) \end{bmatrix}
\]

Logo, vemos que, cada elemento \(y_i\) de \(Y\), onde \(y_i = \varphi(\varpi_i^L \cdot A^{L-1[b]})\), depende estritamente de qualquer \(\varpi_h^{L-1}\) devido ao fato de que todos os elementos \(y_i\) contém \(A^{L-1[b]}\) que contém um \(\varpi_h^{L-1}\).

Então definindo nossa função custo, temos:

\[
\psi_{\text{MSE}} = \frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2, \quad y_i = a_i^L= \varphi(z_i^L)= \varphi\left (\varpi_i^L \cdot \begin{bmatrix} 1 \\ \varphi(\varpi_1^{L-1} \cdot A^{L-2[b]}) \\ \vdots \\ \varphi(\varpi_{k_{L-1}}^{L-1} \cdot A^{L-2[b]})  \end{bmatrix} \right )
\]

Sendo assim podemos definir a variação da função custo com base em \(\varpi_h^{L-1}\). Definimos \(E_i = (y_i - \hat{y_i})^2\), então:

\[
\frac{\partial \psi_{\text{MSE}}}{\partial \varpi_h^{L-1}} = \frac{1}{m} \sum_{i=1}^{m} \frac{\partial E_i }{\partial a_i^L}\frac{\partial a_i^L }{\partial z_i^L}\frac{\partial z_i^L }{\partial a_h^{L-1}}\frac{\partial a_h^{L-1} }{\partial z_h^{L-1}}\frac{\partial z_h^{L-1} }{\partial \varpi_h^{L-1}}
\]

Para a derivada parcial de \(E_i\) e \(a_i^L\) já sabemos o resultado devido a dedução do tópico anterior:

\[
\sum_{i=1}^{m}\frac{\partial E_i}{\partial a_i^L}\frac{\partial a_i^L}{\partial z_i^L} = \sum_{i=1}^{m}2(y_i - \hat{y_i})\dot{\varphi}(z_i^L) = \sum_{i=1}^{m}\delta_i^L
\]

Repare que o somatório se mantem devido ao fato que todos os termos \(i\) dependem de \(\varpi_h^{L-1}\), logo, todos são deriváveis. Agora, nosso objetivo é derivar \(z_i^L\) com base em \(a_h^{L-1}\). 

\[
z_i^L = \varpi_i^L \cdot A^{L-1[b]} = \begin{bmatrix} w_{0[i]}^L & w_{1[i]}^L & \dots & w_{k_{L-1}[i]}^L\end{bmatrix}\begin{bmatrix} a_{0}^{L-1} \\ a_{1}^{L-1} \\ \vdots \\ a_{k_{L-1}}^{L-1}\end{bmatrix} = \sum_{j=0}^{k_{L-1}} w_{j[i]}^{L}a_{j}^{L-1}
\]

Com base no somatório acima, temos:

\[
\frac{\partial}{\partial a_h^{L-1}} \sum_{j=0}^{k_{L-1}} w_{j[i]}^{L}a_{j}^{L-1} = w_{h[i]}^{L}
\]

Tal resolução acima se dá pelo fato que de somente será derivável quando o índice \(j = h\). Concatenando as resoluções já desenvolvidas temos:

\[
\frac{\partial \psi_{\text{MSE}}}{\partial \varpi_h^{L-1}} = \left [ \frac{1}{m} \sum_{i=1}^{m} \delta_i^L w_{h[i]}^L \right ]\frac{\partial a_h^{L-1} }{\partial z_h^{L-1}}\frac{\partial z_h^{L-1} }{\partial \varpi_h^{L-1}}
\]

Agora, finalizando as derivadas parciais para \(a_h^{L-1}\) e \(z_h^{L-1}\) temos:

\[
\frac{\partial a_h^{L-1} }{\partial z_h^{L-1}}\frac{\partial z_h^{L-1} }{\partial \varpi_h^{L-1}} = \dot{\varphi}(z_h^{L-1}) \cdot A^{L-2[b]}
\]

Então:

\[
\frac{\partial \psi_{\text{MSE}}}{\partial \varpi_h^{L-1}} = \left [ \frac{1}{m} \sum_{i=1}^{m} \delta_i^L w_{h[i]}^L \right ]\dot{\varphi}(z_h^{L-1}) \cdot A^{L-2[b]}
\]

Para atualizar os pesos \(\varpi_h^{L-1}\) onde \(h = 1, \dots, k_{L-1}\):

\[
\varpi_{h[r+1]}^{L-1} := \varpi_{h[r]}^{L-1} - \alpha \left [ \frac{1}{m} \sum_{i=1}^{m} \delta_i^L w_{h[i]}^L \right ]\dot{\varphi}(z_h^{L-1}) \cdot A^{L-2[b]}
\]

Para facilitar a compreensão, podemos visualizar graficamente como cada termo ser relaciona:

<img src="/MLP_backpropagation.svg" alt="Output y_i definition"/>

Isso conclui que, diferente dos outputs da camada \(L\) onde temos um valor de referência \(\hat{y_i}\) para cada \(y_i\) para calcular o erro, nos outputs da camada \(L-1\) não temos um valor de referência \(\hat{a}_h^{L-1}\) para calcular o erro de \(a_h^L\). Como base nisso, o que substitui esse processo é a retropropagação, onde o valor de referência do erro é \(\sum_{i=1}^{m}\delta_i^L w_{h[i]}^L\). Isso é propriamente a média dos "erros" da camada \(L\) multiplicado por seu respectivo peso \(w_{h}^L\). 


Para atualizar um peso qualquer \(w_{j[h]}^{L-1}\) onde \(j=0, \dots, k_{L-2[b]}\), que faz parte do vetor de pesos \(\varpi_h^{L-1}\) onde \(\varpi_h^{L-1} \in \mathbb{R}^{k_{L-2[b]}}\) e \(h=1, \dots, k_{L-1[b]}\) temos:

\[
w_{j[h][r+1]}^{L-1} := w_{j[h][r]}^{L-1} - \alpha \left [ \frac{1}{m} \sum_{i=1}^{m} \delta_i^L w_{h[i]}^L\right ] \dot{\varphi}(z_h^{L-1})a_{j}^{L-2}
\]
