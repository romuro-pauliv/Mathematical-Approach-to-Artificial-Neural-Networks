# Mathematical Approach to ANN - Part 1

> Developed by Pauliv, RÃ´mulo. Data scientist, ML/AI Engineer

In the document below, we will explore the development of an artificial neural network $n$-$L$-$m$ ($n$ inputs, $L$ hidden layers, and $m$ outputs), as well as the theoretical learning algorithm based on the weights $\varpi$ of each neuron involved.

In a artificial neural network (ANN) consisting of only one neuron, commonly referred to as a perceptron, we have the following components that constitute a function $f(x) = y$, where $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$:

### Inputs

The inputs will be denoted by the variable $X^t$, where $X^t = \begin{bmatrix}x_1 & x_2 & \dots & x_n\end{bmatrix}$, with each $x_i \in \mathbb{R}$ and for $X$ as a vector, we have $X \in \mathbb{R}^{n \times 1}$. The notation $X^t$ is based on the transposition of the vector $X$. The variable $x_0$ will be reserved to represent the bias in the future; therefore, the real $n$-inputs of the perceptron will always be referred to through $i = 1, \dots, n$.

### Weights

The weights will be responsible for modulating the $n$-inputs from vector $X$. Thus, concerning the first layer, where the inputs will always be the vector $X$, we will always have $\varpi^t = \begin{bmatrix} w_1 & w_2 & \dots & w_n \end{bmatrix}$, where each $w_i \in \mathbb{R}$, and for the vector $\varpi$, we have $\varpi \in \mathbb{R}^{n \times 1}$ with the quantity of $n$-inputs of $X$ equal to the $n$-weights of $\varpi$. The weight $w_0$ will be reserved to represent the bias also. 

### Transfer Function $z(x)$

Thus, $z(X, \varpi)$, where $z: \mathbb{R}^n \times \mathbb{R}^n  \rightarrow \mathbb{R}$ is responsible for relating the $n$-inputs with their respective $n$-weights to generate $z$, serving as the transfer function.

```math
z = \begin{bmatrix} w_1 & w_2 & \dots &  w_n \end{bmatrix} \cdot \begin{bmatrix} x_1\\ x_2 \\ \vdots \\ x_n \end{bmatrix} + b = \sum_{i=1}^{n}x_iw_i + b = \varpi \cdot X+b
```
In the above definition, we have the _affine function_. However, we can also define the bias as $b = w_0$ and $x_0 = 1$ by substituting the bias $b$ in the affine function with a neuron whose weight serves as the bias, and its input is always set to 1. In this method, where the bias acts as a weight, we have a _linear function_.

For the linear function, where \(x_0\) is part of the vector \(X\), we will represent the vector \(X\) as \(X^{[b]}\) where \(X^{[b]} \in \mathbb{R}^{n+1}\). When represented in a dot product with \(\varpi\), it will not be necessary to indicate the addition of \(w_0\) in the vector \(\varpi\) due to the recurring inference that when \(\varpi \cdot X^{[b]}\) exists, there is \(w_0\) for the element \(x_0\), but always remembering that \(\varpi \in \mathbb{R}^{n+1}\) due to the addition of the element \(w_0\). So for \(z(\varpi, X^{[b]})\) where \(z: \mathbb{R}^{n+1} \times \mathbb{R}^{n+1} \rightarrow \mathbb{R}\), we have:

```math
z = \begin{bmatrix} w_0 = b & w_1 & w_2 & \dots &  w_n \end{bmatrix} \cdot \begin{bmatrix} x_0 = 1\\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \sum_{i=0}^{n}x_iw_i = \varpi \cdot X^{[b]}
```

In the graphical representation below, we illustrate the processing of the $n$-inputs $x_i$ through the respective weights $w_i$, resulting in $z$. This is denoted as the _transfer function_.

<img src="/transfer_function_1.svg" alt="The perceptron"/>

### Activation Function $\varphi(z)$
The activation function is responsible for introducing nonlinearity into the system. Therefore, it always receives the value of $z$. The activation function can be defined in various forms such as sigmoid, hyperbolic tangent, rectified linear unit, etc. In this topic, we will not focus on the behavior of each function yet but rather on how the system is structured with it. Thus, for only one layer of neurons with a single neuron, we have $\varphi: \mathbb{R} \rightarrow \Theta $ such that \(\Theta \) represents any set of images depending on the function \(\varphi\):

```math
\varphi(z) = \varphi(\varpi \cdot X) = \varphi(\sum_{i=0}^{n}x_iw_i) = a
```

Note that we can represent the activation function as $\varphi(z) = a$. In this case, as we are constructing a single processing layer with only one neuron, we have $a=y$. Thus, we can represent it as follows:

<img src="/activation_function_2.svg" alt="Single Functional Neuron"/>

So, from the perspective of a perceptron, we can define the function $f(X^{[b]}) = y$, where $f: \mathbb{R}^{n+1} \rightarrow \mathbb{\Theta}$ if the activation function \(\varphi\) is used, otherwise \(f(X^{[b]}) = y\) where \(f: \mathbb{R}^{n+1} \rightarrow \mathbb{R}\):

```math
f(X) = \varphi(\varpi \cdot X) = y
```

