### Multilayer Perceptron (MLP)

Building upon the generalization from the previous section, we can extend the relationship \(A^1 = \varphi(W^1 \cdot X^{[b]})\) to any layer \(L\). Thus, we have:

\[
A^L = \varphi(W^L \cdot A^{L-1 [b]})
\]

Where, when \(L=1\), \(A^{L-1[b]} = X^{[b]}\). Therefore, every \(A^{L} \in \mathbb{\Theta}^{k_L}\), where the dimension \(k_L\) may differ from \(k_{L-1}\) and so forth. Thus, for \(f^L(A^{L-1[b]})\), we have \(f^L: \Theta^{k_{L-1} + 1} \rightarrow \Theta^{k_L}\).

For \(W^L\), we have \(W^L \in \mathbb{R}^{(k_{L-1}+1) \times k}\). To simplify the notation of the vector \(W\), we will denote the additional dimension for the bias \(k_{L-1} + 1\) as \(k_{L-1}[b]\). This can also be applied to the domain dimension of the function \(f^L\). Hence:

\[
f^L(A^{L-1[b]}) = \varphi(W^L \cdot A^{L-1[b]}), \quad f^L: \Theta^{k_{L-1}[b]} \rightarrow \Theta^{k_L} \quad \text{and} \quad W^L \in \mathbb{R}^{k_{L-1}[b] \times k_L}
\]

We can visualize the generalization in the representation below. Notice that throughout the development, we always use the activation function in all neurons. It is also possible, in various cases, to use the ANN without the activation function, but we will not address this format.

<img src="/img/MLP.svg" alt="MLP Input and Hidden Layers"/>

Now, for the last layer \(A^{L}\), referred to as the output of our multilayer perceptron, we can define \(A^L = Y\). The dimension of \(A^L\), which is \(k_L\), will be denoted by \(m\). Therefore, \(Y \in \Theta^m\). Based on these truths, we have:

\[
Y = A^L = \varphi(W^L \cdot A^{L-1[b]})
\]

With the inputs, hidden layers, and outputs defined, we can proceed to a general definition.

__Definition 1__: _A Multilayer perceptron is a function_ \(F: \mathbb{R}^n \rightarrow \Theta^m\). _It is an_ \(n\)-\(L\)-\(m\) _perceptron (_\(n\)_-inputs_, \(L\) _hidden layers_, _and_ \(m\)_outputs) if it is a function of the form_

\[
F(X) = f^L(f^{L-1}(\dots f^1(X))), \quad \text{for} \quad X^t=\begin{bmatrix} x_0 & x_1 & \dots & x_n \end{bmatrix}
\]

_Where \(f^1(X) = \varphi(W^1 \cdot X)\) and for the remaining layers \(f^L(A^{L-1}) = \varphi(W^L \cdot A^{L-1})\)_.