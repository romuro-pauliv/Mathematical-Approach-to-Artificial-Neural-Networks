### Layer with Multiple Neurons

In the previous definition, we had a layer with only one neuron. In this topic, we will define the behavior of a layer with $k$-neurons. Thus, the function $f(X^{[b]})$ will not define only the behavior of one neuron but of $k$-neurons.

Therefore, we will have the definition of $f^L(X^{[b]})$ where $L$ is the index of the layer. Each layer $L$ can have $k_L$ neurons. So, for each neuron $k$, we will have an output $a_{k}^1$ in the layer $f^1(X^{[b]})$.

<img src="/img/layer_with_multiple_neurons.svg" alt="Layer with multiple neurons"/>

in the representation above, the crossed circle will be used to denote a neuron with activation function $\varphi$. When this representation is not used, we refer to the neuron without the activation function, where its output is $z$. Additionally, that the superscript $1$ in all $a_k^1$ refers to the layer we are in. Based on this case, we know that:

\[
\forall a_i^1 \exists \varpi_i^1, \quad i = 1, \dots, k
\]

The term \(a_0^1\) is reserved as the bias of the next layer, where \(a_0^1 = 1\). Thus, we can define the representation of the outputs of the first layer as \(A^1 = \begin{bmatrix} a_1^1 & a_2^1 & \dots & a_k^1 \end{bmatrix}\) where \(A^1 \in \mathbb{\Theta}^k\). When referring to the vector \(A^1\) by adding the bias element \(a_0^1\), we will denote it as \(A^{1[b]}\), where \(A^{1[b]} \in \mathbb{\Theta}^{k+1}\).

Basically, when we are treating the vector \(A^1\) as the output of the layer, the bias \(a_0^1\) will not be added. When we are using the vector \(A^1\) as the input of the next layer, it will be denoted as \(A^{1[b]}\) due to the fact that the bias element \(a_0^1\) exists in the vector.

Now we can represent the layer as a whole based on the following formulation:

\[
A^1 = \begin{bmatrix} a_1 \\ \vdots \\ a_k \end{bmatrix} = \begin{bmatrix} \varphi(\varpi_1^1 \cdot X^{[b]}) \\ \vdots \\ \varphi(\varpi_k^1 \cdot X^{[b]} ) \end{bmatrix}
\]

Therefore, based on this, we can define that:

\[
A^1 =  \varphi \left ( \begin{bmatrix} w_{0[1]}^1 & w_{1[1]}^1 & \dots & w_{n[1]}^1 \\ w_{0[2]}^1 & w_{1[2]}^1 & \dots & w_{n[2]}^1 \\ \vdots & \vdots & \ddots & \vdots \\ w_{0[k]}^1 & w_{1[k]}^1 & \dots & w_{n[k]}^1\end{bmatrix} \begin{bmatrix} x_0 \\ x_1 \\ \vdots \\ x_n \end{bmatrix} \right ) = \varphi(W^1 \cdot X^{[b]})
\]

We define \(W^1\) as a matrix containing all the weights of the first layer, where \(W^1 \in \mathbb{R}^{(n+1) \times k}\). Note that, in the matrix multiplication represented above, we have the dimensions of \(W^{1[t]}\) as \(k \times (n+1)\) and the dimensions of \(X\) as \((n+1) \times 1\), allowing the operation. In the dot product \(W^1 \cdot X^{[b]}\), the theorem of this operation with the proper dimensions of the vectors is intrinsic.

Thus, this allows us to state that:

\[
f^1(X^{[b]}) = A^1, \quad f^1: \mathbb{R}^{n+1} \rightarrow \Theta^{k}
\]

Where the dimension \(k\) refers to the number of neurons in the first layer and \(n\) the number of inputs in this layer.